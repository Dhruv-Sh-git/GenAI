{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce2894bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to create tools\n",
    "# how to use builtin tools like SerpAPI, Wikipedia, etc. and toolkits\n",
    "# how to use chat models to call tools\n",
    "# how to pass tool outputs to chat models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "847c8e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create tool using @tool decorator (defining a custom tool)\n",
    "from langchain.tools import tool\n",
    "@tool\n",
    "async def division(a:int, b:int) -> float:\n",
    "    \"\"\"Divides two numbers.\"\"\"\n",
    "    return a / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fc980983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "division\n",
      "Divides two numbers.\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(division.name)\n",
    "print(division.description)\n",
    "print(division.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0e84cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Annotated,List\n",
    "\n",
    "@tool\n",
    "def multiply_by_max(\n",
    "    a:Annotated[int,\"A value\"],\n",
    "    b:Annotated[List[int],\"B list value\"]\n",
    ")->int:\n",
    "    \"\"\"multiply by maximum\"\"\"\n",
    "    return a*max(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8899064f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "18\n"
     ]
    }
   ],
   "source": [
    "### Structured Tool\n",
    "from langchain_core.tools import StructuredTool\n",
    "def multiply(a:int, b:int) -> int:\n",
    "    \"\"\"Multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "async def amultiply(a:int, b:int) -> int:\n",
    "    \"\"\"Asynchronously multiply two numbers.\"\"\"\n",
    "    return a * b\n",
    "\n",
    "# StructuredTool expects the callable under the `func` argument, not `fun`\n",
    "calculator = StructuredTool.from_function(func=multiply, coroutine=amultiply)\n",
    "print(calculator.invoke({\"a\":2,\"b\":3}))\n",
    "print(await calculator.ainvoke({\"a\":3,\"b\":6}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "14aa590f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "\n",
      "\n",
      "Page: Retrieval-augmented generation\n",
      "Summary: Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retriev\n"
     ]
    }
   ],
   "source": [
    "#using inbuilt tool (e.g. wikipedia)\n",
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "\n",
    "api_wrapper=WikipediaAPIWrapper(top_k_results=5,doc_content_chars_max=500)\n",
    "wiki_tool=WikipediaQueryRun(api_wrapper=api_wrapper)\n",
    "print(wiki_tool.invoke({\"query\":\"langchain\"}))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "67cee4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "242224c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tavily helps in browsing the web\n",
    "os.environ[\"TAVILY_API_KEY\"]=os.getenv(\"TAVILY_API_KEY\")\n",
    "os.environ[\"GROQ_API_KEY\"]=os.getenv(\"GROQ_API_KEY\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ec79e018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_tavily import TavilySearch\n",
    "tavily_tool=TavilySearch(\n",
    "    max_results=5,\n",
    "    topics=[\"technology\",\"science\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d6cfdf6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is the recent AI news??',\n",
       " 'follow_up_questions': None,\n",
       " 'answer': None,\n",
       " 'images': [],\n",
       " 'results': [{'url': 'https://www.crescendo.ai/news/latest-ai-news-and-updates',\n",
       "   'title': 'The Latest AI News and AI Breakthroughs that Matter Most: 2025',\n",
       "   'content': 'Summary: Xiaomi has announced a next-gen AI voice model optimized for in-car and smart home experiences. The model features faster response times, offline',\n",
       "   'score': 0.99943846,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.wsj.com/tech/ai?gaa_at=eafs&gaa_n=AWEtsqd8LHH6MzE2QK0F7OdQv3I-20Nz49id4xEpUEsOzxg3rt8pdGFvzRXl&gaa_ts=694593a7&gaa_sig=jf23knIhqcdMyvD5qX3rhxE0fXp04GQiieWcNifu_t13vCJEZ5OB7HArKRdyRGY8PymtC5INnrgf_lFUsuIbFQ%3D%3D',\n",
       "   'title': 'Artificial Intelligence - Latest AI News and Analysis - WSJ.com',\n",
       "   'content': \"CEOs to Keep Spending on AI, Despite Spotty Returns · Teneo's annual survey finds 68% of chief executives plan to increase AI spending in 2026. By. Ben Glickman.\",\n",
       "   'score': 0.9981614,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.nbcnews.com/artificial-intelligence',\n",
       "   'title': 'Artificial intelligence - NBC News',\n",
       "   'content': \"The latest news and top stories on artificial intelligence, including AI chatbots like Microsoft's ChatGPT, Apple's AI Chatbot and Google's\",\n",
       "   'score': 0.99711037,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.artificialintelligence-news.com/',\n",
       "   'title': 'AI News | Latest News | Insights Powering AI-Driven Business Growth',\n",
       "   'content': \"50,000 Copilot licences for Indian service companies · Zara's use of AI shows how retail workflows are quietly changing · AI in Human Resources: the real\",\n",
       "   'score': 0.99629277,\n",
       "   'raw_content': None},\n",
       "  {'url': 'https://www.reuters.com/technology/artificial-intelligence/',\n",
       "   'title': 'Artificial Intelligence - AI News - Reuters',\n",
       "   'content': 'Explore the latest artificial intelligence news with Reuters - from AI breakthroughs and technology trends to regulation, ethics, business and global',\n",
       "   'score': 0.99583375,\n",
       "   'raw_content': None}],\n",
       " 'response_time': 0.89,\n",
       " 'request_id': 'b5b3d7b8-9b89-45f2-b71b-5d0e1dd5e734'}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tavily_tool.invoke(\"What is the recent AI news??\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df401172",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tools in chat models\n",
    "tools=[tavily_tool,division,wiki_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3a159bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(profile={'max_input_tokens': 131072, 'max_output_tokens': 16384, 'image_inputs': False, 'audio_inputs': False, 'video_inputs': False, 'image_outputs': False, 'audio_outputs': False, 'video_outputs': False, 'reasoning_output': True, 'tool_calling': True}, client=<groq.resources.chat.completions.Completions object at 0x000001D9E1760E00>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001D9E12CDBE0>, model_name='qwen/qwen3-32b', model_kwargs={}, groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##integrating tools with our llm model\n",
    "from langchain.chat_models import init_chat_model\n",
    "llm=init_chat_model(\"qwen/qwen3-32b\",model_provider=\"groq\")\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d571e800",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_tool=llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e768de1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'reasoning_content': 'Okay, let\\'s see. The user is asking \"whatis 2 divided by 3\". First, I need to understand what they\\'re asking. It looks like a simple division problem. The functions available include a division tool, which takes two integers, a and b, and returns a divided by b. Since the user is asking for 2 divided by 3, I should use the division function here. The parameters are a=2 and b=3. I don\\'t need to use any other tools like the search engine or Wikipedia because this is a straightforward arithmetic question. So the correct tool to call is the division function with those values.\\n', 'tool_calls': [{'id': 'tfwcbsdrj', 'function': {'arguments': '{\"a\":2,\"b\":3}', 'name': 'division'}, 'type': 'function'}]} response_metadata={'token_usage': {'completion_tokens': 161, 'prompt_tokens': 1815, 'total_tokens': 1976, 'completion_time': 0.284882255, 'completion_tokens_details': {'reasoning_tokens': 132}, 'prompt_time': 0.093748052, 'prompt_tokens_details': None, 'queue_time': 0.159289707, 'total_time': 0.378630307}, 'model_name': 'qwen/qwen3-32b', 'system_fingerprint': 'fp_2bfcc54d36', 'service_tier': 'on_demand', 'finish_reason': 'tool_calls', 'logprobs': None, 'model_provider': 'groq'} id='lc_run--019b37bb-133b-7a00-9360-61e8e7680415-0' tool_calls=[{'name': 'division', 'args': {'a': 2, 'b': 3}, 'id': 'tfwcbsdrj', 'type': 'tool_call'}] usage_metadata={'input_tokens': 1815, 'output_tokens': 161, 'total_tokens': 1976, 'output_token_details': {'reasoning': 132}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "query=\"whatis 2 divided by 3\"\n",
    "messages=[HumanMessage(query)]\n",
    "response=llm_with_tool.invoke(query)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "74b33872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'division',\n",
       "  'args': {'a': 2, 'b': 3},\n",
       "  'id': 'tfwcbsdrj',\n",
       "  'type': 'tool_call'}]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6a0492c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def execute_tools(response, messages):\n",
    "    for tool_call in response.tool_calls:\n",
    "        selected_tool = {\n",
    "            \"division\": division,\n",
    "            \"tavily_search\": tavily_tool,\n",
    "            \"wikipedia\": wiki_tool\n",
    "        }[tool_call[\"name\"].lower()]\n",
    "\n",
    "        # Run tool\n",
    "        result = await selected_tool.ainvoke(tool_call[\"args\"])\n",
    "\n",
    "        # Wrap result correctly\n",
    "        tool_message = ToolMessage(\n",
    "            content=str(result),\n",
    "            tool_call_id=tool_call[\"id\"]\n",
    "        )\n",
    "\n",
    "        messages.append(tool_message)\n",
    "\n",
    "    return messages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4a266232",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Unsupported message type: <class 'float'>\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/MESSAGE_COERCION_FAILURE ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[58]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m final_response = \u001b[38;5;28;01mawait\u001b[39;00m llm_with_tool.ainvoke(messages)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BIT\\OneDrive\\Desktop\\genai\\.venv\\Lib\\site-packages\\langchain_core\\runnables\\base.py:5561\u001b[39m, in \u001b[36mRunnableBindingBase.ainvoke\u001b[39m\u001b[34m(self, input, config, **kwargs)\u001b[39m\n\u001b[32m   5554\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m   5555\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m   5556\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5559\u001b[39m     **kwargs: Any | \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   5560\u001b[39m ) -> Output:\n\u001b[32m-> \u001b[39m\u001b[32m5561\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bound.ainvoke(\n\u001b[32m   5562\u001b[39m         \u001b[38;5;28minput\u001b[39m,\n\u001b[32m   5563\u001b[39m         \u001b[38;5;28mself\u001b[39m._merge_configs(config),\n\u001b[32m   5564\u001b[39m         **{**\u001b[38;5;28mself\u001b[39m.kwargs, **kwargs},\n\u001b[32m   5565\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BIT\\OneDrive\\Desktop\\genai\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:422\u001b[39m, in \u001b[36mBaseChatModel.ainvoke\u001b[39m\u001b[34m(self, input, config, stop, **kwargs)\u001b[39m\n\u001b[32m    411\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01masync\u001b[39;00m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mainvoke\u001b[39m(\n\u001b[32m    413\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    418\u001b[39m     **kwargs: Any,\n\u001b[32m    419\u001b[39m ) -> AIMessage:\n\u001b[32m    420\u001b[39m     config = ensure_config(config)\n\u001b[32m    421\u001b[39m     llm_result = \u001b[38;5;28;01mawait\u001b[39;00m \u001b[38;5;28mself\u001b[39m.agenerate_prompt(\n\u001b[32m--> \u001b[39m\u001b[32m422\u001b[39m         [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_convert_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m],\n\u001b[32m    423\u001b[39m         stop=stop,\n\u001b[32m    424\u001b[39m         callbacks=config.get(\u001b[33m\"\u001b[39m\u001b[33mcallbacks\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    425\u001b[39m         tags=config.get(\u001b[33m\"\u001b[39m\u001b[33mtags\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    426\u001b[39m         metadata=config.get(\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    427\u001b[39m         run_name=config.get(\u001b[33m\"\u001b[39m\u001b[33mrun_name\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m    428\u001b[39m         run_id=config.pop(\u001b[33m\"\u001b[39m\u001b[33mrun_id\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[32m    429\u001b[39m         **kwargs,\n\u001b[32m    430\u001b[39m     )\n\u001b[32m    431\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    432\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAIMessage\u001b[39m\u001b[33m\"\u001b[39m, cast(\u001b[33m\"\u001b[39m\u001b[33mChatGeneration\u001b[39m\u001b[33m\"\u001b[39m, llm_result.generations[\u001b[32m0\u001b[39m][\u001b[32m0\u001b[39m]).message\n\u001b[32m    433\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BIT\\OneDrive\\Desktop\\genai\\.venv\\Lib\\site-packages\\langchain_core\\language_models\\chat_models.py:377\u001b[39m, in \u001b[36mBaseChatModel._convert_input\u001b[39m\u001b[34m(self, model_input)\u001b[39m\n\u001b[32m    375\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m StringPromptValue(text=model_input)\n\u001b[32m    376\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model_input, Sequence):\n\u001b[32m--> \u001b[39m\u001b[32m377\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatPromptValue(messages=\u001b[43mconvert_to_messages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_input\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    378\u001b[39m msg = (\n\u001b[32m    379\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mInvalid input type \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(model_input)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    380\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mMust be a PromptValue, str, or list of BaseMessages.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    381\u001b[39m )\n\u001b[32m    382\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BIT\\OneDrive\\Desktop\\genai\\.venv\\Lib\\site-packages\\langchain_core\\messages\\utils.py:388\u001b[39m, in \u001b[36mconvert_to_messages\u001b[39m\u001b[34m(messages)\u001b[39m\n\u001b[32m    386\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(messages, PromptValue):\n\u001b[32m    387\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m messages.to_messages()\n\u001b[32m--> \u001b[39m\u001b[32m388\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43m_convert_to_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m messages]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\BIT\\OneDrive\\Desktop\\genai\\.venv\\Lib\\site-packages\\langchain_core\\messages\\utils.py:366\u001b[39m, in \u001b[36m_convert_to_message\u001b[39m\u001b[34m(message)\u001b[39m\n\u001b[32m    364\u001b[39m     msg = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported message type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(message)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    365\u001b[39m     msg = create_message(message=msg, error_code=ErrorCode.MESSAGE_COERCION_FAILURE)\n\u001b[32m--> \u001b[39m\u001b[32m366\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(msg)\n\u001b[32m    368\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m message_\n",
      "\u001b[31mNotImplementedError\u001b[39m: Unsupported message type: <class 'float'>\nFor troubleshooting, visit: https://docs.langchain.com/oss/python/langchain/errors/MESSAGE_COERCION_FAILURE "
     ]
    }
   ],
   "source": [
    "final_response = await llm_with_tool.ainvoke(messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40899d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object RunnableBindingBase.ainvoke at 0x000001D9E16ECE50>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm_with_tool.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d258f13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
